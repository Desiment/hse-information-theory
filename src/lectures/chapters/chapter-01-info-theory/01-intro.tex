% !TeX root = ../../notes.tex
% !TeX spellcheck = ru-RU
% !TeX encoding = UTF-8 Unicode
% !BIB program = biber
% LTeX: language=ru-RU
% LTeX: enablePickyRules=true
% Copyright (c) 2026 Михаил Михайлов
%
% This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0
% International License. To view a copy of this license, visit:
% http://creativecommons.org/licenses/by-nc-sa/4.0/
%
% You are free to:
%   Share - copy and redistribute the material in any medium or format
%   Adapt - remix, transform, and build upon the material
%
% Under the following terms:
%   Attribution - You must give appropriate credit, provide a link to the license,
%                 and indicate if changes were made.
%   NonCommercial - You may not use the material for commercial purposes.
%   ShareAlike - If you remix, transform, or build upon the material, you must
%                distribute your contributions under the same license as the original.
%
% See the LICENSE file in the repository root for full license text.


\section{Введение в теорию информации}

\localtableofcontents

\subsection{Теория информации: предмет, мотивация и исторический контекст}
\textit{Теория информации} --- это раздел математики, возникший в середине XX века в ответ на инженерные и научные задачи передачи сообщений. Её цель состоит в том, чтобы количественно описать информацию и установить фундаментальные ограничения на её представление, сжатие и передачу. Классические вопросы, на которые пытается ответить теория информации, можно сформулировать следующим образом:
\begin{itemize}[compact]
    \item как измерить количество информации в сообщении;
    \item каковы предельные возможности сжатия данных без потерь;
    \item как надёжно передавать сообщения по зашумлённым каналам;
    \item как связаны информация, случайность и неопределённость.
\end{itemize}

	Но что есть информация? Слово \emph{информация} происходит от латинского \textit{informatio}, что означало \enquote{формирование образа, представления}. Информация может передаваться, способна наделять знанием и обуславливать действия, обладать количественным измерением. Изначально оно описывало процесс превращения знания в структуру, которую можно осознать.

Современная теория информации берёт своё начало в работе К.~Шеннона 1948 года, где была предложена математическая модель связи, позволившая отделить физические характеристики канала от абстрактной структуры сообщений. Существенным шагом было осознание того, что смысл передаваемых сообщений не играет роли при анализе предельных возможностей передачи. Именно такой отказ сделал возможным строгий количественный анализ.

Центральная идея теории информации состоит в том, что информация связана с \textit{неопределённостью} исхода. Чтобы прояснить эту связь, полезно рассматривать информацию как меру изменения нашего знания о системе. До получения сообщения мы допускаем несколько возможных состояний мира; после получения сообщения множество допустимых состояний сокращается. Чем сильнее это сокращение, тем более информативным является сообщение.
\begin{example}
    Рассмотрим подбрасывание честной монеты. До эксперимента возможны два равновероятных исхода. После наблюдения результата неопределённость исчезает полностью. В этом смысле сообщение о результате подбрасывания монеты устраняет одну условную единицу неопределённости.
\end{example}
Если исход был практически предопределён заранее, то соответствующее сообщение не приводит к заметному изменению нашего знания.
\begin{example}
    Сообщение \enquote{завтра взойдёт солнце} практически не несёт информации, так как вероятность этого события до получения сообщения была близка к единице.
\end{example}
Иногда про информацию думают как про меру \enquote{удивительности} сообщения.
\begin{example}
    В городе ``П'' солнечная погода может быть только в августе, а во все остальные месяца погода пасмурная каждый день. Сообщение \enquote{Сегодня в городе ``П'' пасмурно} не несет никакой информации. В тоже время, более удивительное (поскольку более редкое) сообщение \enquote{Сегодня в городе ``П'' солнечно} сразу позволяет нам заключить что сегодня --- август.
\end{example}

Такая интерпретация информации тесно связана с вероятностным описанием источника сообщений. Наши ожидания относительно возможных исходов естественным образом выражаются через вероятности. Маловероятные события оказываются более неожиданными и, следовательно, более информативными. Эта идея лежит в основе количественного определения информации, предложенного Шенноном.

Существует и другой, более алгоритмический взгляд на информацию. Интуитивно сообщение можно считать информативным, если его трудно описать короткой инструкцией. Например, последовательность из тысячи нулей легко описывается фразой \enquote{тысяча нулей подряд}, тогда как случайная на вид двоичная строка той же длины не допускает столь компактного описания и требует перечисления почти всех символов. В этом смысле информация связана с минимальной длиной описания объекта.
\begin{example}
	Результат подбрасывания честной монеты можно передать одним битом. Результат броска честного кубика требует большего числа битов, так как число возможных исходов выше и ни один из них не выделен заранее.
\end{example}

Хотя в рамках курса мы не будем формализовывать этот алгоритмический подход, он тесно связан с вероятностной теорией информации и во многом дополняет её. В обоих случаях информация отражает отсутствие простоты, предсказуемости или сжатого описания.
\subsection{Модель коммуникации Шеннона--Уивера}

Первым, кто предложил рассматривать вопросы, связанные с информацией, независимо от её семантического содержания, был Клод Шеннон. Вместе с Уивером он занимался анализом процессов передачи информации, стремясь отделить математические свойства сообщений от их смысла. Для формального исследования передачи информации они предложили универсальную модель коммуникации, которая до сих пор служит фундаментом теории информации. Схема этой модели представлена на \picref{fig::shannon-model}.

% Картинка
\begin{figure}[h]
	\centering
	\scalebox{0.75}{\input{assets/images/01-shannon-model.tikz}}
	\caption{Модель Шенонна-Уивера}
	\label{fig::shannon-model}
\end{figure}
Модель включает пять основных компонентов, которые описывают полный цикл передачи информации:

\begin{itemize}
    \item \textbf{Источник информации} --- объект, генерирующий последовательность символов или сообщений. Источник может быть текстовым, аудио- или видеосигналом, результатом случайного эксперимента или любой другой дискретной последовательностью или функцией от времени.
    \item \textbf{Передатчик (кодирующее устройство)} --- устройство или алгоритм, преобразующий сообщение в форму, пригодную для передачи по каналу. Кодирование здесь не связано с шифрованием, оно служит для представления информации так, чтобы её можно было надежно и эффективно передавать.
    \item \textbf{Канал/источник шума} --- среда передачи сообщений, которая может вносить ошибки, искажения или задержки. Каналом может быть например оптоволоконный кабель или электромагнитное поле по которому распространяются радиоволны.
    \item \textbf{Приёмник (декодирующее устройство)} --- устройство, восстанавливающее исходное сообщение из полученного сигнала. Основная задача декодера --- точно восстановить исходное сообщение.
    \item \textbf{Получатель} --- конечный адресат, для которого предназначено сообщение. В терминах модели именно получатель фиксирует информацию и измеряет её эффект на уменьшение неопределённости.
\end{itemize}

Смысл этой модели в том, что информацию можно измерять количественно, независимо от её содержания. Выход источника рассматривается как случайная величина с определённым распределением вероятностей. Каждое сообщение уменьшает неопределённость относительно исхода, а структура модели позволяет оценить, сколько информации реально передаётся и как её оптимально кодировать.

\begin{example}
Рассмотрим источник, генерирующий буквы $A$ и $B$ с равными вероятностями (у каждого из символов вероятность~$0.5$). Передатчик кодирует символ~$A$ как 0, $B$ как 1. Если канал идеален, приёмник восстанавливает сообщение без ошибок, и каждая буква передаёт одну битовую единицу информации. Если канал зашумлён, некоторые биты могут быть изменены случайным образом. Приёмник получает сигнал с неопределённостью, и задача кодера и декодера состоит в минимизации потерь информации и вероятности ошибки с помощью специальных кодов.
\end{example}

Следующий важный элемент модели --- это понятие \textbf{сигнала}. Сигнал представляет собой физическую или математическую форму, в которой информация передаётся по каналу.  Основное различие делается между дискретными и аналоговыми сигналами. Дискретные сигналы принимают конечное число значений. Примером служит последовательность битов в цифровом сообщении. Аналоговые сигналы непрерывны во времени и амплитуде, как звуковая волна или радиосигнал. Их передача требует дискретизации и квантования, чтобы можно было использовать цифровые методы обработки и кодирования.

\begin{remark}
Квантование и дискретизация --- это два  процесса преобразования аналогового сигнала для цифровой обработки и передачи.

\textbf{Дискретизация} заключается в разбиении непрерывного по времени сигнала на отдельные моменты (выборки), фиксируя его значения через равные интервалы времени. Этот шаг делает сигнал \enquote{дискретным по времени} и позволяет работать с ним как с последовательностью отдельных измерений.

\textbf{Квантование} означает аппроксимацию непрерывных значений амплитуды каждой выборки конечным числом уровней. В результате амплитуда, которая изначально может принимать любое значение, заменяется ближайшим доступным уровнем, делая сигнал дискретным по амплитуде. 
\end{remark}

\newpage
Более детальная классификация сигналов учитывает дискретность по времени и уровню амплитуды (квантование). Различают следующие виды сигналов:
\begin{itemize}
    \item \textbf{Непрерывные (аналоговые) сигналы} --- непрерывны по времени и амплитуде. Пример: чистый аналоговый радиосигнал. Для передачи таких сигналов используют аналоговые каналы или дискретизацию для цифровой обработки.
    \item \textbf{Дискретно-непрерывные сигналы} --- дискретны по времени, амплитуда непрерывна. Пример: показания датчика, снятые через равные интервалы времени. Требует квантования для передачи по цифровому каналу.
    \item \textbf{Непрерывно-квантованные сигналы} --- непрерывны по времени, амплитуда дискретизована. Пример: аналоговый сигнал с периодической выборкой и квантованием для цифровой передачи.
    \item \textbf{Дискретно-квантованные (цифровые) сигналы} --- дискретны по времени и амплитуде. Пример: цифровой аудиопоток, последовательность битов в сети. Такие сигналы \enquote{легко} анализировать и кодировать.
\end{itemize}
На \picref{fig::signal-types} изображены различные формы представления одного и тоже аналогового сигнала.

\begin{figure}[ht]
	\centering
	\scalebox{0.9}{\input{assets/images/02-signal-types-01.tikz}}
	\hspace{0.5cm}
	\scalebox{0.9}{\input{assets/images/02-signal-types-02.tikz}}
	
	\vspace{0.5cm}
	
	\scalebox{0.9}{\input{assets/images/02-signal-types-03.tikz}}
	\hspace{0.5cm}
	\scalebox{0.9}{\input{assets/images/02-signal-types-04.tikz}}

	\caption{Виды сигналов}
	\label{fig::signal-types}
\end{figure}

\begin{example} Несколько примеров сигналов. 
	\begin{itemize}[compact]
		\item Сигнал микрофона, подаваемый напрямую на усилитель без оцифровки, является аналоговым сигналом: и время, и амплитуда изменяются непрерывно.
		\item Температурный датчик, снимающий показания каждую секунду, создаёт дискретно-непрерывный сигнал. Значения температуры меняются непрерывно, но фиксируются через равные промежутки времени.
		\item Температурный датчик, передающий показания непрерывно, но с округлением до доли градуса, наоборот является примером источника квантованного сигнала.
		\item Цифровая аудиозапись в формате WAV является примером цифрового сигнала: звук оцифровывается с определённой частотой дискретизации и квантуется по амплитуде.
	\end{itemize}
\end{example}

\noindent В рамках модели Шеннона--Уивера также можно выделить основные классы задач передачи сообщений:
\begin{itemize}[compact]
    \item \textbf{Сжатие без потерь} --- минимизация объёма передачи без потери информации. Используется, когда необходимо восстановить исходное сообщение точно.
    \item \textbf{Сжатие с потерями} --- уменьшение объёма передачи с допустимой потерей деталей, например, для аудио- и видеосигналов.
    \item \textbf{Избыточное кодирование} --- добавление дополнительных битов или структур, позволяющих обнаруживать и исправлять ошибки, возникающие в канале.
\end{itemize}

Давайте пока в качестве \enquote{toy model} рассмотрим случай цифрового сигнала и на его основе познакомимся с базовыми идеями теории информации. 

\subsection{Собственная информация}
Пусть по каналу связи передаётся одно из заранее заданных сообщений \(s_1,\dots,s_k\). Предполагается, что сообщение~\(s_i\) передаётся с вероятностью~\(p_i\). Каждому событию, состоящему в получении того или иного сообщения, мы хотим сопоставить числовую величину, называемую \emph{собственной информацией} этого события.

Интуитивно ясно, что редкое сообщение должно нести больше информации, чем частое: получение почти гарантированного исхода мало что сообщает получателю, тогда как маловероятный исход существенно сокращает неопределённость. Эти соображения формализуются в виде следующих аксиом.

\vspace{1ex}

\noindent\textbf{Аксиомы собственной информации.}
Пусть \(A\) --- случайное событие. Функция \(\I(A)\), измеряющая информацию, должна удовлетворять условиям:
\begin{enumerate}
  \item\label{si:ax:positivity} \textbf{Неотрицательность:} \(\I(A) \ge 0\) для любого события \(A\) с положительной вероятностью;
  \item\label{si:ax:monotone} \textbf{Монотонность:} при увеличении вероятности \(\P(A)\) величина \(\I(A)\) убывает;
  \item\label{si:ax:additive} \textbf{Аддитивность:} для \textit{независимых} событий \(A\) и \(B\)
  \[
    \I(A \cap B) = \I(A) + \I(B).
  \]
\end{enumerate}
Эти требования отражают базовые свойства информации: она не может быть отрицательной, уменьшается для более ожидаемых событий и суммируется при независимом объединении источников неопределённости.
\begin{remark}
    При этом еще одно требование, уже не столь естественное --- чтобы из равенства $\P(A) = \P(B)$ следовало равенство $\I(A) = \I(B)$. Иначе говоря, информация зависит только от вероятности события. Может показаться странным, что мы оцениваем информацию, не учитывая семантику событий. Однако если в примере из начала положить $s_1 = "0", s_2 = "1"$ (т.е. по каналу передаётся один бит), то в ситуации, когда символы равновероятны, сообщение ``0'' будет настолько \enquote{удивительным}, насколько сообщение ``1''.
\end{remark}

Следующий результат показывает, что выбранные аксиомы практически однозначно определяют вид функции собственной информации.

\begin{Theorem}[Единственность функции собственной информации]
	\label{th:info-uniqness}
	Существует единственная (с точностью до умножения на неотрицательную константу) непрерывная функция \(\I(A)\), зависящая только от вероятности события \(\P(A)\) и удовлетворяющая аксиомам \ref{si:ax:positivity}--\ref{si:ax:additive}.
\end{Theorem}
Для доказательства этой теоремы потребуется вспомогательная лемма.
\begin{lemma}
	\label{lm:cauchy-equation}
    Пусть $f \colon \R_{+} \to \R$ непрерывная и для любых $x, y \in \R_{+}$ выполнено равенство:
    \begin{equation}
        \label{eq:cauchy-equation}
        f(x + y) = f(x) + f(y)
    \end{equation}
    Тогда $f(x) = cx$ для некоторого $c \in \R$.
\end{lemma}
\begin{proof}
    Заметим, что в силу \ref{eq:cauchy-equation} для любого $x \in \R_{+}$ и для любого $n \in \N$ имеем:
    \[
        f(nx) = f(x) + f((n-1)\cdot x) = \ldots = n \cdot f(x)\,.
    \]
    Применяя это равенство к $x = \frac{y}{m}$, где $y \in \R_{+}$ и для любого $m \in \N$ получаем что:
    \[
        f(y) = f\mleft(m \cdot \frac{y}{m}\mright) = m f\mleft(\frac{y}{m}\mright) \quad\Rightarrow\quad  f\mleft(\frac{y}{m}\mright) = \frac{1}{m}f(y)
    \]
    Таким образом для любых $n, m \in \N$ и для любого $x \in \R$:
    \[
         f\mleft(\frac{n}{m} x\mright) = \frac{n}{m}f(x)
    \]
    Положим $c = f(1)$. Докажем что $f(x) = cx$. Пусть $\set{q_i}_{i = 1}^{+\infty}, q_i = \frac{n_i}{m_i}, n_i, m_i \in \N$ --- последовательность рациональных чисел, такая что $q_i \xrightarrow[n \to +\infty]{} x$. По непрерывности $f$:
    \[
        f(q_i) \xrightarrow[n \to +\infty]{} f(x)
    \]
    С другой стороны, в силу доказанного ранее, $f(q_i) = q_i f(1) = c \cdot q_i$. Откуда
    \[
        cx = \lim_{i \to +\infty} (c \cdot q_i) = f(x)
    \]
\end{proof}
\begin{proof}[Доказательство теоремы]
Аддитивность (аксиома \ref{si:ax:additive}) для независимых событий означает, что для любых \(p, q \in (0,1]\) должно выполняться
\[
    \I(pq) = \I(p) + \I(q),
\]
где \(I(p)\) обозначает информацию события вероятности \(p\). Рассмотрим \(f(t)= \I(\exp(-t))\):
\[
    f(t_1 + t_2) = f(t_1) + f(t_2)
\]
И значит \(f(t) = ct\) по лемме \ref{lm:cauchy-equation}. Тогда, поскольку \(\I\) монотонна (аксиома \ref{si:ax:monotone})
\[
    \I(\exp(-t)) = ct \Rightarrow \I^{-1}(ct) = \exp(-t) \Rightarrow \I^{-1}(t) = \exp(-\frac{t}{c}) \Rightarrow \I(p) = -c \ln p
\]
\end{proof}
\begin{remark}
	На самом деле, непрерывность в формулировке теоремы \ref{th:info-uniqness} (и леммы \ref{lm:cauchy-equation}) избыточна и достаточна только монотонность.
\end{remark}
Таким образом, аксиоматический подход естественным образом приводит к логарифмической шкале измерения информации.

\begin{definition}[Собственная информация события]
    Собственной информацией события \(A\) с вероятностью \(\P(A)\) называется величина
    \[
        \I_m(A) = -\log_m \P(A),
    \]
    где \(m > 1\) --- положительная константа.
\end{definition}
\begin{remark}
    Выбор основания логарифма определяет единицы измерения информации: при основании \(2\) информация измеряется в битах, при основании \(e\) --- в натах. Далее, если не оговорено иное, будем использовать логарифмы по основанию \(2\) и обозначать \(\I = \I_2\).
\end{remark}

Рассмотрим несколько простых ситуаций, иллюстрирующих введённое определение.

\begin{example}[Честная монета]
Подбрасывается честная монета. Возможны два исхода: орёл \(O\) и решка \(R\), причём
    \[
        \P(O)= \P(R) = 0.5.
    \]
    Собственная информация каждого исхода равна
    \[
        \I(O)=\I(R)=-\log_2 0.5 = 1 \text{ бит}.
    \]
    Получение любого исхода полностью устраняет неопределённость относительно результата броска.
\end{example}

\begin{example}[Нечестная монета]
    Пусть монета выпадает орлом с вероятностью \(0.8\) и решкой с вероятностью \(0.2\). Тогда
    \[
        \I(O)=-\log_2 0.8 \approx 0.32 \text{ бита}, \qquad
        \I(R)=-\log_2 0.2 \approx 2.32 \text{ бита}.
    \]
    Редкий исход (решка) несёт существенно больше информации, чем ожидаемый.
\end{example}
